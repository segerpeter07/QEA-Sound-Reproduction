\documentclass{tufte-handout}
\usepackage{graphicx}
\usepackage{exercise}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
%\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkbordercolor = {white},
}
\usepackage{tikz}
\usepackage[numbered, framed]{matlab-prettifier}

% Silence warnings from tufte-handout bibliography handling
\usepackage{silence}
\WarningFilter{latex}{File `dummy.bib'}
\begin{filecontents*}{dummy.bib}
\end{filecontents*}
\WarningsOff[natbib]
\WarningsOff[bibentry]
\bibliographystyle{plainnat}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\foo}[1]{%
\begin{tikzpicture}[#1]%
    \includegraphics[width=0.3cm]{figs/MatlabLogo.png}
\end{tikzpicture}%
}
\newcommand{\fuh}[1]{%
\begin{tikzpicture}[#1]%  x
    \includegraphics[width=0.3cm]{figs/MathematicaLogo.png}
\end{tikzpicture}%
}
\newcommand{\fie}[1]{%
\begin{tikzpicture}[#1]%
    \includegraphics[width=0.3cm]{figs/SolidworksLogo.png}
\end{tikzpicture}%
}
\let\ph\mlplaceholder % shorter macro
\lstMakeShortInline"

\lstset{
  style              = Matlab-editor,
  basicstyle         = \mlttfamily,
  escapechar         = ",
  mlshowsectionrules = true,
}


\title{Eigen Math \& Frequency Analysis for Audio}
\author{}
\date{\today}

\begin{document}

\maketitle

\vspace{0.1in}

% ----- Learning Goals -----
\section{Learning Goals}

\begin{description}[font=$\bullet$\scshape\bfseries]

\item[]Understand the difference between correlation and covariance matrix and how they are applied in different situations. 

\item[]Understand what singular vector decomposition yields and learn how to implement SVD in MATLAB.

\item[]Apply the knowledge from facial recognition and implement for voice classification via MATLAB.

\item[]Be able to conduct processing audio data in MATLAB.

\item[]Understand the complex math concept from external sources, such as academic paper.
\end{description}


% ----- Eigen-Math Review -----

\section{Eigen Analysis Revisited (3 hours)}

\subsection{What did we learn?}
During QEA I Module II, we learned how to recognize faces by conducting Principal Component Analysis, also known as Eigenfaces method. Basic idea for voice recognition works in similar fashion: a voice can be classified based on its variation from the entire mean of the voice data. Since \textbf{variance} can be also interpreted as the distance, or difference from the mean of the data, we can apply the same math from Module II for recognizing voices, such as Singular Vector Decomposition. Since it has been to long from middle of the first semester, we will review mathematical concept. 

\subsection{Principal Component Analysis - Correlation and Covariance}
Principal Component Analysis (PCA) uses an orthogonal transformation to convert a data set to a set of values of linearly uncorrelated variables called principal components.The number of distinct principal components is equal to the smaller of the number of original variables or the number of observations minus one (N-1). The transformation is defined that the first principal component has the largest possible variance, second principal component has the second largest possible variance and so on. Since the components are orthogonal to the preceding components, the resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables. Conducting PCA can be done by eigenvalue decomposition of a data covariance or correlation matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing) the data matrix for each elements. (\href{https://en.wikipedia.org/wiki/Principal_component_analysis}{Referenced})

A correlation matrix is a matrix showing correlation coefficients between sets of variables. Higher correlation coefficients mean two different data tips are more related to each other. Therefore, if a data matrix is defined as following:

\[
\textbf{Data} = 
\begin{bmatrix}
    x_{1} & y_{1}  \\
    x_{2} & y_{2}  \\
    \vdots & \vdots \\
    x_{n} & y_{n} 
\end{bmatrix}
\]
The correlation matrix is:
\[
\textbf{Correlation Matrix(A)}= 
\frac{1}{\sqrt{N}}
\begin{bmatrix}
    \frac{x_{1}-\mu_{x}}{\sigma_{x}}  & \frac{y_{1}-\mu_{y}}{\sigma_{y}} \\
    \frac{x_{2}-\mu_{x}}{\sigma_{x}} & \frac{y{2}-\mu_{y}}{\sigma_{y}} \\
    \frac{x_{3}-\mu_{x}}{\sigma_{x}} & \frac{y_{3}-\mu_{y}}{\sigma_{y}} \\
    \dots  & \dots
\end{bmatrix}
\]
\begin{marginfigure}
    \centering
    \includegraphics[width = 6cm, height = 5cm]{correlation.jpg}
    \label{fig:corr}
\caption{Example of Correlation Matrix Plot}
(\href{https://www.quora.com/Whats-the-best-way-to-visualize-correlations-between-six-value-vectors}{Referenced})


\end{marginfigure}

Where N is the number of rows in the matrix, $\mu_{x}, \mu_{y}$ are the mean of the data set, and $\sigma_{x}, \sigma_{y}$ are the standard deviation of the each column. 

A covariance matrix is a matrix built of covariance coefficients, which are the measures of how changes in one variable are associated with changes in a second variable. Essentially, covariance coefficient indicates the degree of linear association between two variables. Therefore, covariance matrix can be defined as

\[
\textbf{Covariance Matrix(B)}= 
\frac{1}{N-1}
\begin{bmatrix}
    (x_{1}-\mu_{1})(x_{1}-\mu{1})  & (y_{1}-\mu_{1})(y_{1}-\mu{1}) \\
    (x_{2}-\mu_{2})(x_{1}-\mu{1}) & (y_{2}-\mu_{2})(x_{2}-\mu{2}) \\
        \dots  & \dots \\
    (x_{n}-\mu_{n})(x_{1}-\mu{1}) & (y_{n}-\mu_{n})(y_{1}-\mu{2}) 

\end{bmatrix}
\]

\begin{marginfigure}
    \centering
    \includegraphics[width = 6cm, height = 5cm]{cov.png}
    \label{fig:cov}
\caption{Example of Covariance Matrix Plot}
(\href{https://www.mathworks.com/matlabcentral/answers/196574-factor-analysis-a-covariance-matrix-is-not-positive-definite}{Referenced})

\end{marginfigure}

Then, how should we choose what matrix to use for Principal Component Analysis? Covariance matrix is commonly used when variables are on the same scale. If the variables are on different scale, correlation matrix is used. For example, if the variables have different units of measure, such as pounds and inches. Since you cannot compare pounds and inches directly, you should use correlation matrix. If you want to conduct PCA on the temperature data for different cities, since the units of measure are the same (Either Fahrenheit or Celsius), covariance matrix should be used. In cases where scales are different, covariance matrix can also adjust the scales of the variables. For example, if you want to conduct PCA on population for different states, using covariance matrix would account for the size of each states.

\textit{Exercise 1}

Load sample data in MATLAB by typing \textit{load hospital}. Then, with \textit{[X = hospital.Weight, hospital.BloodPressure]}, plot the correlation and covariance matrix of the data and find how weight and blood pressure are related. Also, state how correlation and covariance matrix are different.

\begin{marginfigure}
    \centering
    \includegraphics[width = 6cm, height = 5cm]{PCA.png}
    \label{fig:PCA}
\caption{Example of PCA}
(\href{https://people.duke.edu/~ccc14/sta-663/PCASolutions.html}{Referenced})

\end{marginfigure}

\subsection{Principal Component Analysis - Singular Value Decomposition}
Helpful Resource: 
\href{https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf}{CMU CS Theory}

Singular Value decomposition is decomposition of a matrix M (m by n) whose entries come from the field K, which is either the field of real numbers or the field of complex numbers. The decomposition takes the form of ${\displaystyle \mathbf {M} =\mathbf {U} {\boldsymbol {\Sigma }}\mathbf {V} ^{*}}$, where $\mathbf{U}$ is m by m unitary matrix (if  K = ${\displaystyle \mathbb {R} } \mathbb {R}$ , unitary matrices are orthogonal matrices), $\boldsymbol{{\Sigma}}$ is a diagonal m by n matrix with non-negative real numbers on the diagonal, $\mathbf{V}$ is an n by n unitary matrix over K, and
$\mathbf{{V}^{*}}$ is the conjugate transpose of V.

The diagonal entries ${\sigma_{i}}$ of $\mathbf{\Sigma}$ are known as the singular values of M. A common convention is to list the singular values in descending order. In the other words, the first entry of the singular value (eigenvalue) represents how much of eigenvector corresponding to the eigenvalue is represented in the data. The second entry of the singular value represents the second most eminent direction how data is projected and so on. Therefore, by picking certain number of singular value, you can scale your data set and this process is called Principal Component Analysis. (\href{https://en.wikipedia.org/wiki/Singular-value_decomposition}{Referenced})



\textit{Exerise 2}

Load sample data in MATLAB by typing \textit{load hald}. Conduct SVD on one of four variables and plot the resultant data. These resources might be helpful: \href{https://www.mathworks.com/help/matlab/ref/svd.html}{SVD MATLAB documentation} and \href{http://www.matrixlab-examples.com/singular-value-decomposition.html}{SVD MATLAB tutorial}



% ----- Audio Preprocessing -----

\section{Frequency Analysis and Filtering (3 hours)}

\subsection{Discrete Fourier Transform (DFT)}
*This section reviews and builds off section of the overnight from Module 2 Night 3.

Key ideas reviewed:
\begin{itemize}
	\item You can sample a continuous, real-world signal by sampling at discrete time steps using a simple matrix
	\item You can use these more simple, digital responses to analyze a signal
	\item You can convert a signal in the frequency domain back to the time domain
\end{itemize}

Recall that when a computer (or any digital device) samples an audio source, it samples it at a set frequency, or sampling rate. Basically a computer has a special type of clock in it that allows it to perform sampling operations only a certain number of times during a period of time. 

\begin{figure}
	\centering 
	\includegraphics[width=0.7\textwidth]{discrete_time.png}
	\caption{\smaller{A continuous signal (in grey) being sampled at a specified sample rate (red arrows).}}
	\label{fig:discrete_time}
 \end{figure}

As in figure \ref{fig:discrete_time}, a continuous sound signal can be broken up, or sampled, at different points. As you can imagine, if you don't sample at a high enough frequency, you will lose valuable information about your continuous signal. This is called aliasing. As you also might remember, as per the \textit{Nyquist Sampling Theorem}, you only have to sample at twice the frequency of the highest frequency present in your continuous signal to be able to fully reconstruct your continuous signal. 

Once you have turned a analog, continuous signal into a digital, discrete signal in the time domain, you can project it into the frequency domain which allows for rich analysis of the information within the signal. 

If you'd like to watch a quick (4 minutes) refresher video on the DFT, \href{https://www.youtube.com/watch?v=h6QJLx22zrE}{here} is a good video that explains how it works.

Recall that the Discrete Fourier Transform (DFT) works by taking a signal and sampling at a certain frequency $Fs$ and then correlating how much of each sinusoidal basis function is in each sample. 

Let's say our sampled signal looks like \ref{eq:sampled_audio} where $N$ is the time where the signal was sampled:
\begin{equation}\label{eq:sampled_audio}
    x=
\begin{bmatrix}
    x[1]       \\
    x[2]       \\
    \vdots \\
    x[N-1]
\end{bmatrix}
\end{equation}

Now we need to generate a matrix of the sinusoidal basis functions which are all just sinusoidal functions that are dependent on how many samples were taken and were they are in the sample range.
\begin{equation}\label{eq:basis_fns}
    W=\dfrac{1}{\sqrt{N}}
\begin{bmatrix}
    1 & 1 & 1 & \dots  & 1 \\
    1 & \exp(-\frac{2 \pi j}{N})2 & \exp(-\frac{2 \pi j}{N})3 & \dots  & \exp(-\frac{2 \pi j}{N})(N-1) \\
    1 & \exp(-\frac{3 \pi j}{N})2 & \exp(-\frac{2 \pi j}{N})3 & \dots  & \exp(-\frac{2 \pi j}{N}) (N-1) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & \exp(-\frac{(N-1) \pi j}{N})2 & \exp(-\frac{(N-1) \pi j}{N})3 & \dots  & \exp(-\frac{(N-1) \pi j}{N}) (N-1)
\end{bmatrix}
\end{equation}
This vector can then be used to calculate the weights of $W$ on $x$ as
\begin{equation}
a = W x
\end{equation}
Once you get these weights, you can plot them and see the frequency response of the signal in the frequency domain. This is what the \textit{fft()} function does in Matlab.

To covert the signal back to time domain, you can use a similar matrix and the weights to get time function components. 
\begin{equation}\label{eq:basis_fns}
    W^{-1}=\dfrac{1}{\sqrt{N}}
\begin{bmatrix}
    1 & 1 & 1 & \dots  & 1 \\
    1 & \exp(\frac{2 \pi j}{N})2 & \exp(\frac{2 \pi j}{N})3 & \dots  & \exp(\frac{2 \pi j}{N})(N-1) \\
    1 & \exp(\frac{3 \pi j}{N})2 & \exp(\frac{2 \pi j}{N})3 & \dots  & \exp(\frac{2 \pi j}{N}) (N-1) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & \exp(\frac{(N-1) \pi j}{N})2 & \exp(\frac{(N-1) \pi j}{N})3 & \dots  & \exp(\frac{(N-1) \pi j}{N}) (N-1)
\end{bmatrix}
\end{equation}

\subsection{Noise Removal}
One of the more useful operations in audio processing is the filtering of unwanted noise from an audio sample. In the context of voice recognition, this means removing all extraneous sounds - background noise, microphone hiss, recording artifacts - other than the actual voice that we are trying to isolate. This is particularly relevant to the problem of voice recognition, since we want our program to only match based on the qualities of individual voices, not the conditions of their recordings.

In this section, we will be exploring a few methods of removing unwanted noise using the frequency domain. Using $audioread$ in MATLAB, load the sound sample CHANGE FILE NAME. This is a recording of a NASA transmission, but it has been corrupted with two sine wave tones that are playing over the original sample.

\begin{enumerate}
	\item Plot the DFT of the corrupted audio file, and compare it to the clean, original recording (CHANGE FILE NAME). What is different?
	\item How do the frequencies represented in the original audio sample compare to the frequencies added as noise?
\end{enumerate}

One of the simplest filtering techniques is the ideal high-pass and ideal low-pass filters. For an ideal high-pass filter, all frequencies below a certain threshold are completely eliminated from the signal, while frequencies above that threshold are preserved; the opposite is true for an ideal low-pass filter. These can be accomplished either by applying a convolution or by editing the values of the DFT in MATLAB.

\begin{enumerate}
	\setcounter{enumi}{3}
	\item Implement an ideal band-pass filter and apply it to the corrupted NASA transmission. Keep in mind that you don't want to remove the voice from the recording, so you may want to look up what the typical frequency range of a human voice is when determining your threshold frequencies.
	\item Compare the processed recording to the corrupted recording and to the original, uncorrupted recording. What is different? Does your new sample sound different than the clean sample?
	\item What are the limitations of this method of noise filtering?
\end{enumerate}

\subsection{Spectral Subtraction Overview}
As you probably discovered, ideal band-pass filters are very effective at removing background noise from an audio file \textit{provided that it is outside the frequency range of the sounds you want to preserve}. If the noise had been in the frequency range of human speech, it would not have been possible to filter out this way without damaging the original signal.

Spectral subtraction, a different technique which is actually used commonly in audio processing, doesn't have this limitation. It relies on the following assumption:
\begin{equation}
	T[n] = V[n] + N[n]
\end{equation}
Where $T[n]$  is the DFT of the unprocessed sound sample, including unwanted noise, $V[n]$ is the DFT of the voice or other sound sample you want to isolate, and $N[n]$ is some known frequency profile for the background noise you want to remove. If you rearrange this equation, you get the following:
\begin{equation}
	V[n] = T[n] - N[n]
\end{equation}
In other words, to get the voice information on its own, you just need to subtract the unwanted noise from the original sound sample. This is essentially how spectral subtraction works on a high level, and why it can remove background noise indiscriminately of frequency.

There are several useful resources online for spectral subtraction, but \href{http://practicalcryptography.com/miscellaneous/machine-learning/tutorial-spectral-subraction/}{here} is one that I found particularly useful.

\subsection{Spectral Subtraction Implementation}
For this part of the assignment, you will be using spectral subtraction to remove the excess noise from this audio file: CHANGE FILE NAME. Unlike the previous corrupted recording, this sample has some added radio static that spans the entire frequency domain - including the frequencies represented in the voice itself.

An important step to implementing spectral subtraction is slicing the original sample into several smaller chunks. This helps you better identify and define your noise profile and allows you to operate on samples of arbitrary length.

When determining the length of these slices, there are a few things to keep in mind. Each slice needs to contain a enough information to fully represent your background noise, but at the same time you want shorter samples so that you can fit more in a given time period. I used 0.1 seconds, and it seemed to be a good balance of the two.

Additionally, these samples should not be taken exactly end-to-end, but rather overlap somewhat in their position along the original audio track. For instance, if each sample is 0.1 seconds long, you might have samples every 0.05 seconds which overlap by half. This creates smoother transitions between samples and ensures that you don't lose any important data because it happened to lie on the border between two slices of your sample.

\begin{marginfigure}
    \centering
    \includegraphics[width = 6cm, height = 5cm]{sample_1.png}
    \label{fig:sample_data_1}
\caption{An example of how you might sample your data.}
\end{marginfigure}

After you slice your sample, you want to find the magnitude and phase of the DFT for the sample, as well as for a section of the sample that contains only noise (in the case of the noisy NASA transmission, the first approximately 4.5 seconds are noise, while the last 4.5 seconds are speech). You ultimately want to end up with the following four matrices:

\begin{equation}\label{eq:sampled_audio}
    T_M=
\begin{bmatrix}
    T_{MS1}       \\
    T_{MS2}       \\
    \vdots \\
\end{bmatrix}
	T_\theta=
\begin{bmatrix}
    T_{\theta S1}       \\
    T_{\theta S2}       \\
    \vdots \\
\end{bmatrix}
	N_M=
\begin{bmatrix}
    N_{MS1}       \\
    N_{MS2}       \\
    \vdots \\
\end{bmatrix}
	N_\theta=
\begin{bmatrix}
    N_{\theta S1}       \\
    N_{\theta S2}       \\
    \vdots \\
\end{bmatrix}
\end{equation}

Where $T_M$ is a matrix of all the magnitudes of the DFTs of each slice in your sample, $T_\theta$ is a matrix of all the phase information of the DFTs of each slice in your sample, and $N_M$ and $N_\theta$ are similar matrices but sampled from parts of the recording that contain only noise. The MATLAB functions abs and angle will be useful for determining magnitude and phase, respectively, of a matrix of complex numbers.

Next, we want to determine the average frequency profile of the background noise in our audio file. This will functionally act as the N[n] term that you saw in the first equation, and we will later subtract it from our other data matrix. In this case, we can just find an average:
\begin{equation}
	N_{MA} = \frac{1}{n}\sum_{}^{} N_M
\end{equation}

Where $n$ is the number of slices in your sample of background noise. We can then subtract this ``average noise`` from each sample of our full sample magnitude matrix, with a weight:
\begin{equation}
	T_{M_{clean}} = (1-h)T_M - h*N_{MA} = 
	\begin{bmatrix}
    (1-h)T_{MS1} - h*N_{MA}       \\
    (1-h)T_{MS2} - h*N_{MA}       \\
    \vdots \\
\end{bmatrix}
\end{equation}

Where $h$ is a semi-arbitrary ``noise-to-sound ratio`` The higher this value, the more noise you remove, and the more you damage the original signal. I used a value of 0.7, but it is worth experimenting with this to see how it affects your results.

You also want to make sure that your filtered matrix is always at least zero, because it is the magnitude of your complex DFT values (and a negative magnitude doesn't make sense in this context). You can treat it like a piece wise function:
\begin{figure}
	\centering 
	\includegraphics[width=0.7\textwidth]{piecewise.png}
	\label{fig:piecewise}
 \end{figure}
This gives us the magnitudes of the slice DFTs for a filtered recording. We can use these magnitudes, and the phases we recorded earlier, to obtain the complex values of the DFT:
\begin{equation}
	T_{clean} = T_{M_{clean}}e^{jT_\theta}
\end{equation}
From there, you just need to compute the inverse DFT of each sample and combine the time-domain samples in a useful way.
\begin{enumerate}
	\setcounter{enumi}{6}
	\item Plot the filtered and unfiltered sound in time and frequency domain. How did the spectral subtraction affect the signal?
	\item Listen to your filtered sound and to CHANGE FILE NAME How are they different? Try adjusting your parameter for slice length and see how that affects this difference.
	\item Apply this technique to CHANGE FILE NAME. How does it compare to the ideal band-pass filter for filtering out the two sinewaves?
	\item Try one of the modifications to spectral subtraction described in the ``Modifications`` section of the \href{http://practicalcryptography.com/miscellaneous/machine-learning/tutorial-spectral-subraction/}{linked page}. How does this effect the quality of noise removal?
\end{enumerate}

% ----- Audio Preprocessing -----

\section{Audio Preprocessing (2 hours)}

Now that we've learned how to build two different types of filters, we're going to learn how to apply them to audio clips. As we get into that, however, we will rapidly realize the length of the word matters greatly. Since each person recording their clip didn't say their word at exactly the same time, we are going to need to correct this by trimming the clips to all start at exactly the same time and last for the same duration as can be see in figure \ref{fig:time_graphs}.
\begin{marginfigure}
	\includegraphics[width = 6cm, height = 5cm]{time_1.png}
	\includegraphics[width = 6cm, height = 5cm]{time_2.png}
	\caption{Two raw audio clips before processing}
	\label{fig:time_graphs}
\end{marginfigure}

\subsection{Algorithm Creation}
To start trimming these clips to length, we should start by observing them. Obviously the levels of the word are significantly higher than other times during the clip. However, some clips have more fluctuation in their levels surrounding the words, so we'll need to apply our filtering.


\begin{marginfigure}
	\includegraphics[width = 6cm, height = 5cm]{cleaned_time_2.png}
	\caption{Audio clip after being filtered with a band-pass filter}
	\label{fig:cleaned_time_graphs}
\end{marginfigure}
After filtering is done (as can be seen in figure \ref{fig:cleaned_time_graphs}), after applying the band pass filter, our data is significantly cleaner and devoid of noise. This helps our 'word part' stand out much more.

To isolate the word consistently, we shall define a threshold and gather a section of the clip surrounding the point where this threshold is triggered. In essence, we will look through all the sample points of the signal and when the level goes above 0.1, we shall take out a sub clip of 0.5s surrounding that point. 


\textit{Exerise 1}

Build an algorithm to trim all the sound clips to be the same length and contain the word part. Work on this for an hour, and if you do not come up with something, feel free to use the sample script found \href{#}{here}. We don't want you to struggle on a relatively simple algorithm, but feel it would help familiarize you with the format of the data sets. 

\subsection{Finding Average Voice & Going Further}
% Finding Average Face
\textit{Exercise 2}

Just as you did in the \textit{Eigen Analysis Revisited} section, find the average voice of the data set. (Hint: Start by putting all the sound clips in the frequency domain. Remember to use your cleaned data set that you just made!)

\\
% Go further
\textit{Exercise 3 *Optional}

Now that you have found the average voice, feel free to try the eigen analysis section using the sound data sets and the average voice you just found. 

\end{document}



